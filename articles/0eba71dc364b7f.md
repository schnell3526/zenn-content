---
title: "ãƒ‘ãƒ©ãƒ¡ã‚¿æ•°1.5Bã®gpt2ã‚’å…¬é–‹ã—ãŸè©±"
emoji: "ğŸ”¥"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ['è‡ªç„¶è¨€èªå‡¦ç†','gpt']
published: false
---
å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ä¸­ã§æœ€å¤§è¦æ¨¡ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯rinnaç¤¾ã®æä¾›ã™ã‚‹
[japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)ã§ã€ãƒ‘ãƒ©ãƒ¡ã‚¿æ•°ã¯1.3Bã§ã™ã€‚
è‡ªå‰ã§äº‹å‰å­¦ç¿’ã‚’ã™ã‚‹ã¨ãã®å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

:::details å­¦ç¿’ç’°å¢ƒã«é–¢ã—ã¦
|||
|-|-|
|OS|Ubuntu 20.04 LTS|
|gpu|NVIDIA A100 * 8|
|cpu|Intel(R) Xeon(R) Platinum 8368 CPU @ 2.40GHz * 2|
|ãƒ¡ãƒ¢ãƒª|460gb|

ã”è¦§ã®é€šã‚Šã‹ãªã‚Šãƒã‚¤ã‚¨ãƒ³ãƒ‰ãªç’°å¢ƒã§å‰å‡¦ç†ã‚„å­¦ç¿’ã‚’é€²ã‚ã¦ã„ã‚‹ã®ã§ä¸€èˆ¬çš„ãªç’°å¢ƒã§ã®å‡¦ç†ã¯é›£ã—ã„å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚
:::

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç”¨æ„
ãŠè©¦ã—ãªã®ã§ã€æ—¥æœ¬èªwikipediaã¨cc100ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ã„ã¾ã™ã€‚æ—¥æœ¬èªwikipediaã¯2022/07/01æ™‚ç‚¹ã®ã‚‚ã®ã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚
```shell
wget http://data.statmt.org/cc-100/ja.txt.xz
wget https://dumps.wikimedia.org/jawiki/20220701/jawiki-20220701-pages-articles-multistream.xml.bz2

xz -d ja.txt.xz
bzip2 -d jawiki-20220701-pages-articles-multistream.xml.bz2
```

æ—¥æœ¬èªwikipediaãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’ã—ã¦ã„ãã¾ã™ã€‚
```shell
python3 -m venv .env
source .env/bin/activate

# xmlã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
pip install wikiextractor==3.0.4
wikiextractor jawiki-20220701-pages-articles-multistream.xml
find text/ | grep wiki | awk '{system("cat "$0" >> wiki.txt")}'
rm -rf text/
# æ®‹ã£ãŸã‚¿ã‚°ã‚’é™¤å»
sed -i 's/<[^>]*>//g' wiki.txt
# ç©ºè¡Œã‚’é™¤å»
sed -i '/^$/d' wiki.txt
```

æ¬¡ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ã—ã€è¡Œå˜ä½ã§åˆ†å‰²ã—ã€jumanppã§åˆ†ã‹ã¡æ›¸ãã‚’ã™ã‚‹ã¨ã„ã†å‡¦ç†ã‚’ä¸€åº¦ã«ã‚„ã£ã¦ã„ãã¾ã™ã€‚


```python:split.py
import sys

from textformatting import ssplit
from tqdm.auto import tqdm
import neologdn
input = sys.stdin.readline

MAX_LINES=int(sys.argv[1])

def main():
    line = 'start'

    bar = tqdm(total = MAX_LINES)
    while line:
        line = neologdn.normalize(input().rstrip())
        [print(s) for s in ssplit(line) ]
        bar.update(1)

if __name__ == "__main__":
    main()

```

ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ã„ãŸã‚‰ã€parallelã‚’åˆ©ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®jumanppã«ã‚ˆã‚‹è§£æã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã€‚ã ã„ãŸã„5åˆ†ã§çµ‚ã‚ã‚Šã¾ã—ãŸã€‚

```shell
cat wiki.txt | python split.py 15768565 | parallel --pipe -L 10000 --blocksize 1772096 jumanpp --segment > wiki_mrph.txt
```

cc100ã«é–¢ã—ã¦ã‚‚åŒæ§˜ã§ã™ã€‚ã“ã¡ã‚‰ã¯3æ™‚é–“å¼±ã‹ã‹ã‚Šã¾ã—ãŸã€‚
```shell
cat ja.txt | python split.py 392774277 | parallel --pipe -L 10000 --blocksize 1772096 jumanpp --segment > cc100_mrph.txt
```

# Tokenizerã®å­¦ç¿’
tokenizerã«é–¢ã—ã¦ã‚‚1ã‹ã‚‰å­¦ç¿’ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚äº‹å‰åˆ†å‰²ã¯æ¸ˆã¾ã›ã¦ã„ã‚‹ã®ã§ã€ã„ã‚ã‚†ã‚‹ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ç”¨ã®èªå½™ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
gptã¯å…ƒã€…ã¯byte level BPEã‚’ä½¿ã†ã¨ã®ã“ã¨ã§ã™ãŒä»Šå¾Œã‚‚huggingfaceã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã«é ¼ã£ã¦å­¦ç¿’ã‚’é€²ã‚ã¦ã„ãã®ã§huggingface Tokenizersã§æä¾›ã•ã‚Œã¦ã„ã‚‹(byteãƒ¬ãƒ™ãƒ«ã§ãªã„)BPEå®Ÿè£…ã‚’åˆ©ç”¨ã—ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚

::: details train_tokenizer.py
`files`ã«ã¯äº‹å‰ã«åˆ†ã‹ã¡æ›¸ãã‚’è¡Œã£ãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¾ã™ã€‚

```python:train_tokenizer.py
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(BPE(unk_token="<unk>"))
tokenizer.pre_tokenizer = Whitespace()
trainer = BpeTrainer(
    vocab_size=32000,
    min_frequency=1,
    special_tokens=["<unk>", "<s>", "</s>"],
    limit_alphabet=5000,
    )
files = ["wiki_mrph.txt", "cc100_mrph.txt"]
tokenizer.train(files, trainer)
tokenizer.save("juman-bpe-wiki-cc100.json")
```

ç°¡å˜ã«èª¬æ˜ã™ã‚‹ã¨
pre_tokenizerã«ã‚ˆã£ã¦è¨­å®šã•ã‚Œã‚‹å¢ƒç•Œã¯å¿…ãšå˜èªã®å¢ƒç•Œã¨ãªã‚Šã¾ã™ã€‚ ã‚¹ãƒšãƒ¼ã‚¹ãŒå¿…ãšå˜èªå¢ƒç•Œã¨ãªã‚‹ã‚ˆã†ãªè¨­å®šã‚’ã—ã¦ã„ã¾ã™ã€‚ä»Šå›ã®ä¾‹ã§ã¯äº‹å‰ã«juman++ã§å˜èªå¢ƒç•Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚Œã¦ã„ã‚‹ã®ã§ã€juman++ã«ã‚ˆã£ã¦è¨­å®šã•ã‚ŒãŸå˜èªå¢ƒç•ŒãŒå¿…ãšæœ€çµ‚çš„ãªå˜èªå¢ƒç•Œã«å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ã¾ã™ã€‚

`BpeTrainer`ã«é–¢ã—ã¦ã€èªå½™æ•°ã¯æœ¬å®¶ã®gpt-2ã‚’å‚è€ƒã«50000ã¨è¨­å®šã—ã¾ã—ãŸã€‚`limit_alphabet`ã¯èªå½™ã§ä½¿ç”¨ã™ã‚‹æ–‡å­—æ•°ã§ã™æ—¥æœ¬èªã®å ´åˆã¯æ‰±ã†æ–‡å­—ã®ç¨®é¡ãŒè±Šå¯Œãªã®ã§ã‚ã‚‹ç¨‹åº¦å¤§ããªå€¤ã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä»Šå›ã¯8000ã¨ã—ã¾ã—ãŸã€ã‚‚ã†å°‘ã—å°ã•ãã¦ã‚‚è‰¯ã„æ°—ãŒã—ã¾ã™ã€‚
:::

```shell
python train_tokenizer.py
```


# gpt2ãŒå·¨å¤§ã™ãã‚‹å•é¡Œ
```python
from transformers import CONFIG_MAPPING
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer
from transformers import Trainer

tokenizer = AutoTokenizer.from_pretrained('gpt2-tokenizer-wiki-cc100')
input = tokenizer('ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚', return_tensors='pt')

config = CONFIG_MAPPING['gpt2']()
# ã“ã‚Œã§æœ€å¤§ã‚µã‚¤ã‚º
config.update_from_string("vocab_size=32000,bos_token_id=1,eos_token_id=2,n_embd=1600,n_layer=48,n_head=20")

model = AutoModelForCausalLM.from_config(config)
# 48å€‹ã®attentionã‚’ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ã™ã‚‹
device_map = {
    0: [0, 1, 2, 3, 4, 5, 6, 7, 8],
    1: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],
    2: [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],
    3: [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],
}
model.parallelize(device_map)
```