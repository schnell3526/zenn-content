---
title: "パラメタ数1.5Bのgpt2を公開した話"
emoji: "🔥"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ['自然言語処理','gpt']
published: false
---
公開されている中で最大規模の言語モデルはrinna社の提供する
[japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)で、パラメタ数は1.3Bです。
自前で事前学習をするときの参考にしてください。

:::details 学習環境に関して
|||
|-|-|
|OS|Ubuntu 20.04 LTS|
|gpu|NVIDIA A100 * 8|
|cpu|Intel(R) Xeon(R) Platinum 8368 CPU @ 2.40GHz * 2|
|メモリ|460gb|

ご覧の通りかなりハイエンドな環境で前処理や学習を進めているので一般的な環境での処理は難しい可能性が高いです。
:::

# 学習データの用意
お試しなので、日本語wikipediaとcc100を学習データとして使います。日本語wikipediaは2022/07/01時点のものを利用しました。
```shell
wget http://data.statmt.org/cc-100/ja.txt.xz
wget https://dumps.wikimedia.org/jawiki/20220701/jawiki-20220701-pages-articles-multistream.xml.bz2

xz -d ja.txt.xz
bzip2 -d jawiki-20220701-pages-articles-multistream.xml.bz2
```

日本語wikipediaデータの前処理をしていきます。
```shell
python3 -m venv .env
source .env/bin/activate

# xmlからテキストを抽出
pip install wikiextractor==3.0.4
wikiextractor jawiki-20220701-pages-articles-multistream.xml
find text/ | grep wiki | awk '{system("cat "$0" >> wiki.txt")}'
rm -rf text/
# 残ったタグを除去
sed -i 's/<[^>]*>//g' wiki.txt
# 空行を除去
sed -i '/^$/d' wiki.txt
```

次にテキストを正規化し、行単位で分割し、jumanppで分かち書きをするという処理を一度にやっていきます。


```python:split.py
import sys

from textformatting import ssplit
from tqdm.auto import tqdm
import neologdn
input = sys.stdin.readline

MAX_LINES=int(sys.argv[1])

def main():
    line = 'start'

    bar = tqdm(total = MAX_LINES)
    while line:
        line = neologdn.normalize(input().rstrip())
        [print(s) for s in ssplit(line) ]
        bar.update(1)

if __name__ == "__main__":
    main()

```

上のファイルを書いたら、parallelを利用してボトルネックのjumanppによる解析を並列化する。だいたい5分で終わりました。

```shell
cat wiki.txt | python split.py 15768565 | parallel --pipe -L 10000 --blocksize 1772096 jumanpp --segment > wiki_mrph.txt
```

cc100に関しても同様です。こちらは3時間弱かかりました。
```shell
cat ja.txt | python split.py 392774277 | parallel --pipe -L 10000 --blocksize 1772096 jumanpp --segment > cc100_mrph.txt
```

# Tokenizerの学習
tokenizerに関しても1から学習する必要があります。事前分割は済ませているので、いわゆるサブワード分割用の語彙を学習することになります。
gptは元々はbyte level BPEを使うとのことですが今後もhuggingfaceエコシステムに頼って学習を進めていくのでhuggingface Tokenizersで提供されている(byteレベルでない)BPE実装を利用しようと思います。

::: details train_tokenizer.py
`files`には事前に分かち書きを行ったテキストのパスを指定します。

```python:train_tokenizer.py
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(BPE(unk_token="<unk>"))
tokenizer.pre_tokenizer = Whitespace()
trainer = BpeTrainer(
    vocab_size=32000,
    min_frequency=1,
    special_tokens=["<unk>", "<s>", "</s>"],
    limit_alphabet=5000,
    )
files = ["wiki_mrph.txt", "cc100_mrph.txt"]
tokenizer.train(files, trainer)
tokenizer.save("juman-bpe-wiki-cc100.json")
```

簡単に説明すると
pre_tokenizerによって設定される境界は必ず単語の境界となります。 スペースが必ず単語境界となるような設定をしています。今回の例では事前にjuman++で単語境界にスペースを入れているので、juman++によって設定された単語境界が必ず最終的な単語境界に含まれるようになってます。

`BpeTrainer`に関して、語彙数は本家のgpt-2を参考に50000と設定しました。`limit_alphabet`は語彙で使用する文字数です日本語の場合は扱う文字の種類が豊富なのである程度大きな値を設定する必要があります。今回は8000としました、もう少し小さくても良い気がします。
:::

```shell
python train_tokenizer.py
```


# gpt2が巨大すぎる問題
```python
from transformers import CONFIG_MAPPING
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer
from transformers import Trainer

tokenizer = AutoTokenizer.from_pretrained('gpt2-tokenizer-wiki-cc100')
input = tokenizer('おはようございます。今日は良い天気ですね。', return_tensors='pt')

config = CONFIG_MAPPING['gpt2']()
# これで最大サイズ
config.update_from_string("vocab_size=32000,bos_token_id=1,eos_token_id=2,n_embd=1600,n_layer=48,n_head=20")

model = AutoModelForCausalLM.from_config(config)
# 48個のattentionをモデル並列する
device_map = {
    0: [0, 1, 2, 3, 4, 5, 6, 7, 8],
    1: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],
    2: [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],
    3: [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],
}
model.parallelize(device_map)
```